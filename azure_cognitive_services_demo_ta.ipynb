{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-textanalytics\n",
      "  Downloading azure_ai_textanalytics-5.1.0b4-py2.py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: msrest>=0.6.0 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from azure-ai-textanalytics) (0.6.19)\n",
      "Requirement already satisfied: six>=1.6 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from azure-ai-textanalytics) (1.15.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from azure-ai-textanalytics) (1.1.26)\n",
      "Collecting azure-core<2.0.0,>=1.8.2\n",
      "  Downloading azure_core-1.10.0-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (1.3.0)\n",
      "Requirement already satisfied: requests~=2.16 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (2020.6.20)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from msrest>=0.6.0->azure-ai-textanalytics) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.0->azure-ai-textanalytics) (3.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.6.0->azure-ai-textanalytics) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.6.0->azure-ai-textanalytics) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hcsra95\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.6.0->azure-ai-textanalytics) (1.25.11)\n",
      "Installing collected packages: azure-core, azure-ai-textanalytics\n",
      "Successfully installed azure-ai-textanalytics-5.1.0b4 azure-core-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org azure-ai-textanalytics --pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define subscription endpoint and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"PLEASE_ENTER_YOUR_OWN_KEY\"\n",
    "endpoint = \"https://PLEASE_ENTER_YOUR_OWN_ENDPOINT_NAME.cognitiveservices.azure.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1 in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Sentiment: positive\n",
      "Overall scores: positive=1.00; neutral=0.00; negative=0.00 \n",
      "\n",
      "Sentence: I had the best day of my life.\n",
      "Sentence 1 sentiment: positive\n",
      "Sentence score:\n",
      "Positive=1.00\n",
      "Neutral=0.00\n",
      "Negative=0.00\n",
      "\n",
      "Sentence: I wish you were there with me.\n",
      "Sentence 2 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.21\n",
      "Neutral=0.77\n",
      "Negative=0.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentiment_analysis_example(client):\n",
    "\n",
    "    documents = [\"I had the best day of my life. I wish you were there with me.\"]\n",
    "    response = client.analyze_sentiment(documents=documents)[0]\n",
    "    print(\"Document Sentiment: {}\".format(response.sentiment))\n",
    "    print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "        response.confidence_scores.positive,\n",
    "        response.confidence_scores.neutral,\n",
    "        response.confidence_scores.negative,\n",
    "    ))\n",
    "    for idx, sentence in enumerate(response.sentences):\n",
    "        print(\"Sentence: {}\".format(sentence.text))\n",
    "        print(\"Sentence {} sentiment: {}\".format(idx+1, sentence.sentiment))\n",
    "        print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "            sentence.confidence_scores.positive,\n",
    "            sentence.confidence_scores.neutral,\n",
    "            sentence.confidence_scores.negative,\n",
    "        ))\n",
    "          \n",
    "sentiment_analysis_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2 in Tranditional Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Sentiment: neutral\n",
      "Overall scores: positive=0.05; neutral=0.92; negative=0.03 \n",
      "\n",
      "Sentence: 新冠肺炎疫情反彈，消息指，本港今日（18日）新增超過100宗確診個案，恐繼12月19日以來，再次超越過百宗確診.\n",
      "Sentence 1 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.01\n",
      "Neutral=0.97\n",
      "Negative=0.02\n",
      "\n",
      "Sentence: 截至昨日，本港累計病例有9,558宗.\n",
      "Sentence 2 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.06\n",
      "Neutral=0.91\n",
      "Negative=0.03\n",
      "\n",
      "Sentence: 消息指，北區醫院一名內科男醫生，近日被調到深切治療部工作，曾照顧確診患者，    並為危殆病人施行插喉等高風險醫療程序.\n",
      "Sentence 3 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.07\n",
      "Neutral=0.87\n",
      "Negative=0.06\n",
      "\n",
      "Sentence: 他昨晚(17日)因發燒，自行到該院急症室求診，今早初步確診新冠肺炎，    或成首名確診的深切治療部醫生。九巴表示，一名車長確診，曾駕駛過61M、252及259E共3條路線，已在本月14日休假，    九巴已即時安排所有相關的巴士消毒.\n",
      "Sentence 4 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.04\n",
      "Neutral=0.94\n",
      "Negative=0.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentiment_analysis_example(client):\n",
    "\n",
    "    documents = [\"新冠肺炎疫情反彈，消息指，本港今日（18日）新增超過100宗確診個案，恐繼12月19日以來，再次超越過百宗確診. \\\n",
    "    截至昨日，本港累計病例有9,558宗. 消息指，北區醫院一名內科男醫生，近日被調到深切治療部工作，曾照顧確診患者，\\\n",
    "    並為危殆病人施行插喉等高風險醫療程序. 他昨晚(17日)因發燒，自行到該院急症室求診，今早初步確診新冠肺炎，\\\n",
    "    或成首名確診的深切治療部醫生。九巴表示，一名車長確診，曾駕駛過61M、252及259E共3條路線，已在本月14日休假，\\\n",
    "    九巴已即時安排所有相關的巴士消毒.\"]\n",
    "    response = client.analyze_sentiment(documents=documents)[0]\n",
    "    print(\"Document Sentiment: {}\".format(response.sentiment))\n",
    "    print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "        response.confidence_scores.positive,\n",
    "        response.confidence_scores.neutral,\n",
    "        response.confidence_scores.negative,\n",
    "    ))\n",
    "    for idx, sentence in enumerate(response.sentences):\n",
    "        print(\"Sentence: {}\".format(sentence.text))\n",
    "        print(\"Sentence {} sentiment: {}\".format(idx+1, sentence.sentiment))\n",
    "        print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "            sentence.confidence_scores.positive,\n",
    "            sentence.confidence_scores.neutral,\n",
    "            sentence.confidence_scores.negative,\n",
    "        ))\n",
    "          \n",
    "sentiment_analysis_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Sentiment: neutral\n",
      "Overall scores: positive=0.08; neutral=0.87; negative=0.05 \n",
      "\n",
      "Sentence: 港府防疫無能，未能遏止新冠肺炎疫情在社區蔓延，早前更「斬腳趾避沙蟲」取消全港年宵市場，    連累市民新年無處買年花應節，花農辛勤一年卻損失慘重.\n",
      "Sentence 1 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.05\n",
      "Neutral=0.92\n",
      "Negative=0.03\n",
      "\n",
      "Sentence: 有花農透露，鑑於社會對取消年宵花市反應極大，    政府考慮將花年移師至各區戶外球場重新舉辦.\n",
      "Sentence 2 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.09\n",
      "Neutral=0.85\n",
      "Negative=0.06\n",
      "\n",
      "Sentence: 花農質疑沒有人會到球場買年花，批評政府愚弄花農.\n",
      "Sentence 3 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.10\n",
      "Neutral=0.85\n",
      "Negative=0.05\n",
      "\n",
      "Sentence: 食物環境衞生署回覆指，在防疫抗疫的前提下，該署正積極考慮合適方案協助花農銷售年花，並會分散人流及壓縮規模來減低聚眾風險.\n",
      "Sentence 4 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.08\n",
      "Neutral=0.86\n",
      "Negative=0.06\n",
      "\n",
      "Sentence: 有「桃花大王」之稱的花農劉海濤表示，上星期政府曾與花農業界開會，會上多名花農狠批政府取消年宵市場令到大批準備應市的年花滯銷，    花農於過去一年辛苦裁花的心血盡廢.\n",
      "Sentence 5 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.12\n",
      "Neutral=0.84\n",
      "Negative=0.04\n",
      "\n",
      "Sentence: 他指，政府眼見業界怨聲載道，拋出替代方案「補鑊」，建議於多區的戶外球場重新舉辦花市，    讓受影響花農重新有地方賣花，不至於血本無歸.\n",
      "Sentence 6 sentiment: neutral\n",
      "Sentence score:\n",
      "Positive=0.05\n",
      "Neutral=0.92\n",
      "Negative=0.03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentiment_analysis_example(client):\n",
    "\n",
    "    documents = [\"港府防疫無能，未能遏止新冠肺炎疫情在社區蔓延，早前更「斬腳趾避沙蟲」取消全港年宵市場，\\\n",
    "    連累市民新年無處買年花應節，花農辛勤一年卻損失慘重. 有花農透露，鑑於社會對取消年宵花市反應極大，\\\n",
    "    政府考慮將花年移師至各區戶外球場重新舉辦. 花農質疑沒有人會到球場買年花，批評政府愚弄花農. \\\n",
    "    食物環境衞生署回覆指，在防疫抗疫的前提下，該署正積極考慮合適方案協助花農銷售年花，並會分散人流及壓縮規模來減低聚眾風險. \\\n",
    "    有「桃花大王」之稱的花農劉海濤表示，上星期政府曾與花農業界開會，會上多名花農狠批政府取消年宵市場令到大批準備應市的年花滯銷，\\\n",
    "    花農於過去一年辛苦裁花的心血盡廢. 他指，政府眼見業界怨聲載道，拋出替代方案「補鑊」，建議於多區的戶外球場重新舉辦花市，\\\n",
    "    讓受影響花農重新有地方賣花，不至於血本無歸.\"]\n",
    "    response = client.analyze_sentiment(documents=documents)[0]\n",
    "    print(\"Document Sentiment: {}\".format(response.sentiment))\n",
    "    print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "        response.confidence_scores.positive,\n",
    "        response.confidence_scores.neutral,\n",
    "        response.confidence_scores.negative,\n",
    "    ))\n",
    "    for idx, sentence in enumerate(response.sentences):\n",
    "        print(\"Sentence: {}\".format(sentence.text))\n",
    "        print(\"Sentence {} sentiment: {}\".format(idx+1, sentence.sentiment))\n",
    "        print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "            sentence.confidence_scores.positive,\n",
    "            sentence.confidence_scores.neutral,\n",
    "            sentence.confidence_scores.negative,\n",
    "        ))\n",
    "          \n",
    "sentiment_analysis_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opinion mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Sentiment: positive\n",
      "Overall scores: positive=0.84; neutral=0.00; negative=0.16 \n",
      "\n",
      "Sentence: The food and service were unacceptable, but the concierge were nice\n",
      "Sentence sentiment: positive\n",
      "Sentence score:\n",
      "Positive=0.84\n",
      "Neutral=0.00\n",
      "Negative=0.16\n",
      "\n",
      "......'negative' aspect 'food'\n",
      "......Aspect score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' opinion 'unacceptable'\n",
      "......Opinion score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' aspect 'service'\n",
      "......Aspect score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'negative' opinion 'unacceptable'\n",
      "......Opinion score:\n",
      "......Positive=0.01\n",
      "......Negative=0.99\n",
      "\n",
      "......'positive' aspect 'concierge'\n",
      "......Aspect score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "......'positive' opinion 'nice'\n",
      "......Opinion score:\n",
      "......Positive=1.00\n",
      "......Negative=0.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sentiment_analysis_with_opinion_mining_example(client):\n",
    "\n",
    "    documents = [\n",
    "        \"The food and service were unacceptable, but the concierge were nice\"\n",
    "    ]\n",
    "\n",
    "    result = client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "    doc_result = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    positive_reviews = [doc for doc in doc_result if doc.sentiment == \"positive\"]\n",
    "    negative_reviews = [doc for doc in doc_result if doc.sentiment == \"negative\"]\n",
    "\n",
    "    positive_mined_opinions = []\n",
    "    mixed_mined_opinions = []\n",
    "    negative_mined_opinions = []\n",
    "\n",
    "    for document in doc_result:\n",
    "        print(\"Document Sentiment: {}\".format(document.sentiment))\n",
    "        print(\"Overall scores: positive={0:.2f}; neutral={1:.2f}; negative={2:.2f} \\n\".format(\n",
    "            document.confidence_scores.positive,\n",
    "            document.confidence_scores.neutral,\n",
    "            document.confidence_scores.negative,\n",
    "        ))\n",
    "        for sentence in document.sentences:\n",
    "            print(\"Sentence: {}\".format(sentence.text))\n",
    "            print(\"Sentence sentiment: {}\".format(sentence.sentiment))\n",
    "            print(\"Sentence score:\\nPositive={0:.2f}\\nNeutral={1:.2f}\\nNegative={2:.2f}\\n\".format(\n",
    "                sentence.confidence_scores.positive,\n",
    "                sentence.confidence_scores.neutral,\n",
    "                sentence.confidence_scores.negative,\n",
    "            ))\n",
    "            for mined_opinion in sentence.mined_opinions:\n",
    "                aspect = mined_opinion.aspect\n",
    "                print(\"......'{}' aspect '{}'\".format(aspect.sentiment, aspect.text))\n",
    "                print(\"......Aspect score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                    aspect.confidence_scores.positive,\n",
    "                    aspect.confidence_scores.negative,\n",
    "                ))\n",
    "                for opinion in mined_opinion.opinions:\n",
    "                    print(\"......'{}' opinion '{}'\".format(opinion.sentiment, opinion.text))\n",
    "                    print(\"......Opinion score:\\n......Positive={0:.2f}\\n......Negative={1:.2f}\\n\".format(\n",
    "                        opinion.confidence_scores.positive,\n",
    "                        opinion.confidence_scores.negative,\n",
    "                    ))\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "          \n",
    "sentiment_analysis_with_opinion_mining_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:  French\n"
     ]
    }
   ],
   "source": [
    "def language_detection_example(client):\n",
    "    try:\n",
    "        documents = [\"Ce document est rédigé en Français.\"]\n",
    "        response = client.detect_language(documents = documents, country_hint = 'us')[0]\n",
    "        print(\"Language: \", response.primary_language.name)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "language_detection_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:  Chinese_Traditional\n"
     ]
    }
   ],
   "source": [
    "def language_detection_example(client):\n",
    "    try:\n",
    "        documents = [\"今料增逾100宗個案 傳北區醫院深切治療部醫生染疫\"]\n",
    "        response = client.detect_language(documents = documents, country_hint = 'us')[0]\n",
    "        print(\"Language: \", response.primary_language.name)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "language_detection_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "\n",
      "\tText: \t trip \tCategory: \t Event \tSubCategory: \t None \n",
      "\tConfidence Score: \t 0.61 \n",
      "\n",
      "\tText: \t Seattle \tCategory: \t Location \tSubCategory: \t GPE \n",
      "\tConfidence Score: \t 0.82 \n",
      "\n",
      "\tText: \t last week \tCategory: \t DateTime \tSubCategory: \t DateRange \n",
      "\tConfidence Score: \t 0.8 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def entity_recognition_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"I had a wonderful trip to Seattle last week.\"]\n",
    "        result = client.recognize_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Named Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tText: \\t\", entity.text, \"\\tCategory: \\t\", entity.category, \"\\tSubCategory: \\t\", entity.subcategory,\n",
    "                    \"\\n\\tConfidence Score: \\t\", round(entity.confidence_score, 2), \"\\n\")\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_recognition_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked Entities:\n",
      "\n",
      "\tName:  Microsoft \tId:  Microsoft \tUrl:  https://en.wikipedia.org/wiki/Microsoft \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Microsoft\n",
      "\t\tConfidence Score: 0.55\n",
      "\t\tText: Microsoft\n",
      "\t\tConfidence Score: 0.55\n",
      "\tName:  Bill Gates \tId:  Bill Gates \tUrl:  https://en.wikipedia.org/wiki/Bill_Gates \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Bill Gates\n",
      "\t\tConfidence Score: 0.63\n",
      "\t\tText: Gates\n",
      "\t\tConfidence Score: 0.63\n",
      "\tName:  Paul Allen \tId:  Paul Allen \tUrl:  https://en.wikipedia.org/wiki/Paul_Allen \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Paul Allen\n",
      "\t\tConfidence Score: 0.60\n",
      "\tName:  April 4 \tId:  April 4 \tUrl:  https://en.wikipedia.org/wiki/April_4 \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: April 4\n",
      "\t\tConfidence Score: 0.32\n",
      "\tName:  BASIC \tId:  BASIC \tUrl:  https://en.wikipedia.org/wiki/BASIC \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: BASIC\n",
      "\t\tConfidence Score: 0.33\n",
      "\tName:  Altair 8800 \tId:  Altair 8800 \tUrl:  https://en.wikipedia.org/wiki/Altair_8800 \n",
      "\tData Source:  Wikipedia\n",
      "\tMatches:\n",
      "\t\tText: Altair 8800\n",
      "\t\tConfidence Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "def entity_linking_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"\"\"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, \n",
    "        to develop and sell BASIC interpreters for the Altair 8800. \n",
    "        During his career at Microsoft, Gates held the positions of chairman,\n",
    "        chief executive officer, president and chief software architect, \n",
    "        while also being the largest individual shareholder until May 2014.\"\"\"]\n",
    "        result = client.recognize_linked_entities(documents = documents)[0]\n",
    "\n",
    "        print(\"Linked Entities:\\n\")\n",
    "        for entity in result.entities:\n",
    "            print(\"\\tName: \", entity.name, \"\\tId: \", entity.data_source_entity_id, \"\\tUrl: \", entity.url,\n",
    "            \"\\n\\tData Source: \", entity.data_source)\n",
    "            print(\"\\tMatches:\")\n",
    "            for match in entity.matches:\n",
    "                print(\"\\t\\tText:\", match.text)\n",
    "                print(\"\\t\\tConfidence Score: {0:.2f}\".format(match.confidence_score))\n",
    "            \n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "entity_linking_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKey Phrases:\n",
      "\t\t cat\n",
      "\t\t veterinarian\n"
     ]
    }
   ],
   "source": [
    "def key_phrase_extraction_example(client):\n",
    "\n",
    "    try:\n",
    "        documents = [\"My cat might need to see a veterinarian.\"]\n",
    "\n",
    "        response = client.extract_key_phrases(documents = documents)[0]\n",
    "\n",
    "        if not response.is_error:\n",
    "            print(\"\\tKey Phrases:\")\n",
    "            for phrase in response.key_phrases:\n",
    "                print(\"\\t\\t\", phrase)\n",
    "        else:\n",
    "            print(response.id, response.error)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n",
    "        \n",
    "key_phrase_extraction_example(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
